#!/command/with-contenv bash
set -e

echo "=== Starting vLLM service ==="

# Auto-detect model path if not set
if [ -z "$VLLM_MODEL" ]; then
    MODEL_DIR=$(find /app/models -maxdepth 1 -type d ! -path /app/models | head -1)
    if [ -n "$MODEL_DIR" ]; then
        export VLLM_MODEL="$MODEL_DIR"
        echo "Auto-detected model: $VLLM_MODEL"
    fi
fi

sleep 3

if command -v nvidia-smi &> /dev/null; then
    echo "GPU detected:"
    nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv || true
else
    echo "WARNING: No NVIDIA GPU detected"
fi

echo "vLLM configuration:"
echo "  Model: $VLLM_MODEL"
echo "  Host: $VLLM_HOST"
echo "  Port: $VLLM_PORT"
echo "  GPU Memory: $VLLM_GPU_MEMORY_UTILIZATION"
echo "  Tensor Parallel Size: $VLLM_TENSOR_PARALLEL_SIZE"

if [ -z "$VLLM_MODEL" ]; then
    echo "ERROR: VLLM_MODEL not configured"
    exit 1
fi

echo "Starting vLLM..."

exec vllm serve "$VLLM_MODEL" \
    --host "$VLLM_HOST" \
    --port "$VLLM_PORT" \
    --gpu-memory-utilization "$VLLM_GPU_MEMORY_UTILIZATION" \
    --tensor-parallel-size "${VLLM_TENSOR_PARALLEL_SIZE:-1}" \
    --trust-remote-code
